{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a718503b0efeb06",
   "metadata": {},
   "source": [
    "# Embedding Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043e15e-f487-4c9d-af48-1d243bb4de3a",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "9a7f7f75fb0f0a27",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "acb1112d-2e44-4e2d-a3c1-447bb79e76e7",
   "metadata": {},
   "source": [
    "# ==== MNIST ========\n",
    "#run = \"mnist_MLP_32_0.9595\"\n",
    "#run = \"mnist_CNN_64_0.9901\"\n",
    "\n",
    "# ==== CIFAR 10 ========\n",
    "\n",
    "#run = \"cifar10_MLP_128_0.5672\"\n",
    "\n",
    "run = \"cifar10_CNN_128_0.8977\"\n",
    "\n",
    "#run = \"cifar10_ViT_128_0.6311\"\n",
    "#run = \"cifar10_ViT_128_0.6530_noisy\"\n",
    "\n",
    "# ==== CIFAR 100 ========\n",
    "#run = \"cifar100_CNN_128_0.4991\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f9069f3b27bec4",
   "metadata": {},
   "source": [
    "from data_manager import load_training_data\n",
    "\n",
    "results = load_training_data(run)\n",
    "results[\"embedding_drifts\"] = {int(k): results[\"embedding_drifts\"][k] for k in sorted(results[\"embedding_drifts\"].keys(), key=int)}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1aed3bbcc1aedeb1",
   "metadata": {},
   "source": [
    "## Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3162757bed44022",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from train_viz import _plot_loss_accuracy, _plot_gradients, _plot_embedding_drift\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "epochs = len(results[\"train_losses\"])\n",
    "_plot_loss_accuracy(axs[0][0], epochs-1, epochs, results[\"train_losses\"], results[\"val_losses\"], results[\"train_accuracies\"], results[\"val_accuracies\"])\n",
    "_plot_gradients(axs[0][1], range(0, len(results[\"gradient_norms\"])),results[\"gradient_norms\"], results[\"max_gradients\"], results[\"grad_param_ratios\"], 20)\n",
    "_plot_embedding_drift(axs[1][0], results[\"embedding_drifts\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77560e55ca0c21fa",
   "metadata": {},
   "source": [
    "%matplotlib widget\n",
    "from train_viz import _plot_embedding_drift\n",
    "\n",
    "embedding_drifts = results[\"embedding_drifts\"].copy()\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "# Plot 2x Drifts\n",
    "# axs.plot(range(1, len(embedding_drifts[1]) + 1), np.array(embedding_drifts[1]) * 2, color=\"green\", label=\"2x Drift 1\", alpha=0.3)\n",
    "# axs.plot(range(1, len(embedding_drifts[2]) + 1), np.array(embedding_drifts[2]) * 2, color=\"blue\", label=\"2x Drift 2\", alpha=0.3)\n",
    "# axs.plot(range(1, len(embedding_drifts[4]) + 1), np.array(embedding_drifts[4]) * 2, color=\"orange\", label=\"2x Drift 2\", alpha=0.3)\n",
    "# axs.plot(range(1, len(embedding_drifts[8]) + 1), np.array(embedding_drifts[8]) * 2, color=\"red\", label=\"2x Drift 2\", alpha=0.3)\n",
    "# _plot_embedding_drift(axs, embedding_drifts)\n",
    "# \n",
    "# plt.legend()\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0bbd2ae9-66b7-4acd-abf2-d12a28a3699b",
   "metadata": {},
   "source": [
    "# Visualizations PCA"
   ]
  },
  {
   "cell_type": "code",
   "id": "50f60595-3b4b-4519-b6bb-f2903735f737",
   "metadata": {},
   "source": [
    "from train_viz import generate_projections, animate_projections, show_with_slider, show_multiple_projections_with_slider, visualization_drift_vs_embedding_drift, denoise_projections"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bbdfdc0a0d65a0b0",
   "metadata": {},
   "source": [
    "projections_pca_first = generate_projections(\n",
    "    embeddings_list=results[\"subset_embeddings\"],\n",
    "    method='pca',\n",
    "    pca_fit_basis='first',\n",
    ")\n",
    "projections_pca_last = generate_projections(\n",
    "    embeddings_list=results[\"subset_embeddings\"],\n",
    "    method='pca',\n",
    "    pca_fit_basis='last',\n",
    ")\n",
    "projections_pca_all = generate_projections(\n",
    "    embeddings_list=results[\"subset_embeddings\"],\n",
    "    method='pca',\n",
    "    pca_fit_basis='all',\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb57ff24-549b-460f-9016-cb0cef227687",
   "metadata": {},
   "source": [
    "projections_pca_window = generate_projections(\n",
    "    embeddings_list=results[\"subset_embeddings\"],\n",
    "    method='pca',\n",
    "    pca_fit_basis='window',\n",
    "    window_size=5,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2ccd243-6ec1-4695-aa28-e8636568e7f9",
   "metadata": {},
   "source": [
    "def show_with_slider(\n",
    "        projections,\n",
    "        labels,\n",
    "        figsize=(5, 5),\n",
    "        cmap='tab10',\n",
    "        dot_size=5,\n",
    "        alpha=0.5,\n",
    "        interpolate=False,\n",
    "        steps_per_transition=10\n",
    "    ):\n",
    "    projections = np.array(projections)\n",
    "\n",
    "    # Interpolate if requested\n",
    "    if interpolate:\n",
    "        projections_interp = []\n",
    "        for a, b in zip(projections[:-1], projections[1:]):\n",
    "            for alpha_step in np.linspace(0, 1, steps_per_transition, endpoint=False):\n",
    "                interp = (1 - alpha_step) * a + alpha_step * b\n",
    "                projections_interp.append(interp)\n",
    "        projections_interp.append(projections[-1])\n",
    "    else:\n",
    "        projections_interp = projections\n",
    "\n",
    "    projections = projections_interp\n",
    "\n",
    "    # Setup figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Axis limits\n",
    "    all_proj = np.concatenate(projections, axis=0)\n",
    "    max_abs = np.max(np.abs(all_proj))\n",
    "    ax.set_xlim(-max_abs, max_abs)\n",
    "    ax.set_ylim(-max_abs, max_abs)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Initial scatter\n",
    "    scatter = ax.scatter(projections[0][:, 0], projections[0][:, 1],\n",
    "                         c=labels[0], cmap=cmap, s=dot_size, alpha=alpha)\n",
    "\n",
    "    # Add legend\n",
    "    unique_labels = np.unique(labels[0])\n",
    "    cmap_instance = plt.get_cmap(cmap, len(unique_labels))\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                          label=str(lbl),\n",
    "                          markerfacecolor=cmap_instance(i),\n",
    "                          markersize=6)\n",
    "               for i, lbl in enumerate(unique_labels)]\n",
    "    ax.legend(handles=handles, title=\"Classes\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Slider and update\n",
    "    def update(frame_idx):\n",
    "        scatter.set_offsets(projections[frame_idx])\n",
    "        scatter.set_array(np.array(labels[0]))\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    slider = widgets.Play(min=0, max=len(projections)-1, step=1)\n",
    "    slider_control = widgets.IntSlider(min=0, max=len(projections)-1, step=1)\n",
    "    widgets.jslink((slider, 'value'), (slider_control, 'value'))\n",
    "\n",
    "    out = widgets.interactive_output(update, {'frame_idx': slider_control})\n",
    "    display(widgets.VBox([widgets.HBox([slider, slider_control]), out]))\n",
    "\n",
    "\n",
    "def filter_classes(projections, labels, selected_classes):\n",
    "    projections_filtered = []\n",
    "    labels_filtered = []\n",
    "\n",
    "    for proj, lbl in zip(projections, labels):\n",
    "        mask = np.isin(lbl, selected_classes)\n",
    "        projections_filtered.append(proj[mask])\n",
    "        labels_filtered.append(lbl[mask])\n",
    "\n",
    "    return projections_filtered, labels_filtered"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72f006fe-a074-4041-a4c8-b2c25c0a7792",
   "metadata": {},
   "source": [
    "results[\"subset_labels\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9ccd4186-83d7-4d9e-bb18-b3a84def0cff",
   "metadata": {},
   "source": [
    "show_with_slider(\n",
    "    projections_pca_window,\n",
    "    labels=results[\"subset_labels\"],\n",
    "    interpolate=False,\n",
    "    steps_per_transition=2,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0e852934-5ddf-4f7d-801e-3b076855d3ec",
   "metadata": {},
   "source": [
    "show_multiple_projections_with_slider(\n",
    "    projections_list=[projections_pca_first, projections_pca_last, projections_pca_all, projections_pca_window],\n",
    "    labels=results[\"subset_labels\"],\n",
    "    titles=[\"PCA on first\", \"PCA on last\", \"PCA on all\", \"PCA sliding window\"],\n",
    "    interpolate=True,\n",
    "    steps_per_transition=1,\n",
    "    figsize_per_plot=(4, 4),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "540c6ff0-740e-4572-8701-87e29778428c",
   "metadata": {},
   "source": [
    "visualization_drift_vs_embedding_drift(projections_pca_window, embedding_drifts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "934c08f1-10a9-472c-b8e3-742a70e0d44e",
   "metadata": {},
   "source": [
    "import matplotlib\n",
    "matplotlib.pyplot.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fdb32e30-aa2d-46c1-8c95-33b5c6a1a0b1",
   "metadata": {},
   "source": [
    "# Denoising"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8a4b616-b51b-4cb8-b80b-f7adda205f1c",
   "metadata": {},
   "source": [
    "projections = projections_pca_window\n",
    "#projections = projections_pca_all"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "293d75f8-b0e0-475f-86c3-53fc1c70ced4",
   "metadata": {},
   "source": [
    "visualization_drift_vs_embedding_drift(projections, embedding_drifts, verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4462f544-dc07-437e-a044-bd7d2970f908",
   "metadata": {},
   "source": [
    "denoised_window = denoise_projections(projections, window_size=15, blend=0.9, mode='window')\n",
    "denoised_exponential = denoise_projections(projections, blend=0.8, mode='exponential')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fa84aa3-9d18-4e28-8809-cddd8fff7acf",
   "metadata": {},
   "source": [
    "show_multiple_projections_with_slider(\n",
    "    projections_list=[projections, denoised_window, denoised_exponential],\n",
    "    labels=results[\"subset_labels\"],\n",
    "    titles=[\"PCA\", \"denoised window\", \"denoised exponential\"],\n",
    "    interpolate=False,\n",
    "    figsize_per_plot=(4, 4),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f51d9be1-59e3-4d1d-8575-836d67857a54",
   "metadata": {},
   "source": [
    "denoised_embeddings = denoise_projections(results[\"subset_embeddings\"], window_size=15, blend=0.9, mode='window')\n",
    "visualization_drift_vs_embedding_drift(denoised_window, denoised_embeddings, verbose=True, embeddings=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7094c2c1-21bb-4935-87b2-1a189ffa8f3b",
   "metadata": {},
   "source": [
    "denoised_exponential = denoise_projections(projections, blend=0.8, mode='exponential')\n",
    "denoised_embeddings = denoise_projections(results[\"subset_embeddings\"], blend=0.8, mode='exponential')\n",
    "\n",
    "visualization_drift_vs_embedding_drift(denoised_exponential, denoised_embeddings, verbose=True, embeddings=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "806a0dd8-c141-4c2c-9ffa-674bbd8146e5",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define parameters\n",
    "window_sizes = [1, 2, 4, 6, 8, 10, 15]\n",
    "blend_values = np.linspace(0, 1, 11)\n",
    "correlation_results = {ws: [] for ws in window_sizes}\n",
    "exponentials = []\n",
    "\n",
    "# Run correlations\n",
    "for blend in blend_values:\n",
    "    for ws in window_sizes:\n",
    "        denoised = denoise_projections(projections, window_size=ws, blend=blend, mode='window')\n",
    "        denoised_embeddings = denoise_projections(results[\"subset_embeddings\"], window_size=ws, blend=blend, mode='window')\n",
    "        corr = visualization_drift_vs_embedding_drift(denoised, denoised_embeddings, verbose=False, embeddings=True)\n",
    "        correlation_results[ws].append(corr)\n",
    "        \n",
    "    denoised = denoise_projections(projections, blend=blend, mode='exponential')\n",
    "    denoised_embeddings = denoise_projections(results[\"subset_embeddings\"], blend=blend, mode='exponential')\n",
    "    corr = visualization_drift_vs_embedding_drift(denoised, denoised_embeddings, verbose=False, embeddings=True)\n",
    "    exponentials.append(corr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9ed2ac73-acf6-4edc-93fa-eef1f0dd3c8c",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(8, 4))\n",
    "for ws in window_sizes:\n",
    "    plt.plot(blend_values, correlation_results[ws], label=f'window_size={ws}')\n",
    "plt.plot(blend_values, exponentials, label=f'exponential', linewidth=3)\n",
    "plt.xlabel(\"Blend\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.title(\"Correlation vs. Blend for different denoise calculations\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0cd96e10-327c-41d4-bad2-cc191c546db7",
   "metadata": {},
   "source": [
    "# Define parameters\n",
    "window_sizes = range(1, 20)\n",
    "correlation_results = []\n",
    "blend = 0.9\n",
    "\n",
    "# Run correlations\n",
    "for ws in window_sizes:\n",
    "    denoised = denoise_projections(projections, window_size=ws, blend=blend, mode='window')\n",
    "    denoised_embeddings = denoise_projections(results[\"subset_embeddings\"], window_size=ws, blend=blend, mode='window')\n",
    "    corr = visualization_drift_vs_embedding_drift(denoised, denoised_embeddings, verbose=False, embeddings=True)\n",
    "    correlation_results.append(corr)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(window_sizes, correlation_results)\n",
    "plt.xlabel(\"Window Size\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.title(\"Correlation vs. Window Size\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b69e7fb1-b764-4e82-8636-740d1d0bb4b0",
   "metadata": {},
   "source": [
    "# t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "851a7c6a-4a82-4bb4-8ae2-7c67d3cc223a",
   "metadata": {},
   "source": [
    "projections_tsne = generate_projections(\n",
    "    embeddings_list=results[\"subset_embeddings\"],\n",
    "    method='tsne',\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "799e3c83-35e9-4133-8c5c-18dfdfeb8d19",
   "metadata": {},
   "source": [
    "show_multiple_projections_with_slider(\n",
    "    projections_list=[denoised_exponential, projections_tsne],\n",
    "    labels=results[\"subset_labels\"],\n",
    "    titles=[\"PCA\", \"t-SNE\"],\n",
    "    interpolate=False,\n",
    "    steps_per_transition=2,\n",
    "    figsize_per_plot=(5, 5),\n",
    "    shared_axes=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d0dd05f-88e8-46e0-a5f4-056ccf2ddaff",
   "metadata": {},
   "source": [
    "visualization_drift_vs_embedding_drift(projections_tsne, embedding_drifts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aaea932b-041b-4447-961d-e1c215772c31",
   "metadata": {},
   "source": [
    "denoised_tsne = denoise_projections(projections_tsne, blend=0.8, mode='exponential')\n",
    "denoised_embeddings = denoise_projections(results[\"subset_embeddings\"], blend=0.8, mode='exponential')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a115049-8ef5-479e-afb2-5b3b87766ee4",
   "metadata": {},
   "source": [
    "show_multiple_projections_with_slider(\n",
    "    projections_list=[denoised_exponential, denoised_tsne, projections_tsne],\n",
    "    labels=results[\"subset_labels\"],\n",
    "    titles=[\"PCA\", \"t-SNE denoised\", \"t-SNE\"],\n",
    "    interpolate=False,\n",
    "    steps_per_transition=1,\n",
    "    figsize_per_plot=(5, 5),\n",
    "    shared_axes=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce924bba-7cff-41d5-a92b-4ebaba98ea41",
   "metadata": {},
   "source": [
    "visualization_drift_vs_embedding_drift(denoised_tsne, denoised_embeddings, precalculated=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b2abbe99-489c-4ffb-8148-9a116a277d88",
   "metadata": {},
   "source": [
    "# td-SNE"
   ]
  },
  {
   "cell_type": "code",
   "id": "dab4b7bd-937a-4d05-bec8-2ef18ccc229f",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "\n",
    "\n",
    "class DynamicTSNE:\n",
    "    def __init__(\n",
    "            self,\n",
    "            output_dims=2,\n",
    "            verbose=True,\n",
    "    ):\n",
    "        self.output_dims = output_dims\n",
    "        self.verbose = verbose\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def compute_affinities(self, Xs, perplexity=30.0, k_neighbors=90):\n",
    "        def Hbeta(D, beta):\n",
    "            P = torch.exp(-D * beta)\n",
    "            sumP = torch.sum(P)\n",
    "            sumP = torch.clamp(sumP, min=1e-8)\n",
    "            H = torch.log(sumP) + beta * torch.sum(D * P) / sumP\n",
    "            P = torch.clamp(P / sumP, min=1e-8)\n",
    "            return H, P\n",
    "\n",
    "        def compute_P(X, init_beta=None):\n",
    "            t0 = time.time()\n",
    "\n",
    "            n = X.shape[0]\n",
    "            D = torch.cdist(X, X, p=2).pow(2)\n",
    "            P = torch.zeros((n, n), device=X.device)\n",
    "            beta = init_beta.clone() if init_beta is not None else torch.ones(n, device=X.device)\n",
    "            logU = torch.log(torch.tensor(perplexity, device=X.device))\n",
    "            all_tries = 0\n",
    "\n",
    "            for i in range(n):\n",
    "                distances = D[i]\n",
    "                topk = torch.topk(distances, k=k_neighbors + 1, largest=False)\n",
    "                idx = topk.indices[topk.indices != i][:k_neighbors]\n",
    "                Di = torch.clamp(distances[idx], max=1e3)\n",
    "\n",
    "                betamin, betamax = None, None\n",
    "                H, thisP = Hbeta(Di, beta[i])\n",
    "                tries = 0\n",
    "                while torch.abs(H - logU) > 1e-5 and tries < 50:\n",
    "                    if H > logU:\n",
    "                        betamin = beta[i].clone()\n",
    "                        beta[i] = beta[i] * 2 if betamax is None else (beta[i] + betamax) / 2\n",
    "                    else:\n",
    "                        betamax = beta[i].clone()\n",
    "                        beta[i] = beta[i] / 2 if betamin is None else (beta[i] + betamin) / 2\n",
    "                    H, thisP = Hbeta(Di, beta[i])\n",
    "                    tries += 1\n",
    "                all_tries += tries\n",
    "                P[i, idx] = thisP\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Total affinity computation time: {time.time() - t0:.2f}s, {all_tries / n} Tries\")\n",
    "\n",
    "            P = (P + P.T) / (2 * n)\n",
    "            return P, beta\n",
    "\n",
    "        X_tensor = [torch.tensor(X, device=self.device) for X in Xs]\n",
    "        self.Xs = X_tensor\n",
    "\n",
    "        Ps = []\n",
    "        prev_beta = None\n",
    "        for X in X_tensor:\n",
    "            P, prev_beta = compute_P(X, prev_beta)\n",
    "            Ps.append(P)\n",
    "\n",
    "        self.Ps = torch.stack(Ps)\n",
    "        assert not torch.isnan(self.Ps).any(), \"Affinity matrix has NaN\"\n",
    "\n",
    "    def fit(self, n_epochs=1000, exaggeration=12.0, exaggeration_epochs=250, lr=200.0, lambd=0.1):\n",
    "        T = len(self.Xs)\n",
    "        n = self.Xs[0].shape[0]\n",
    "\n",
    "        Y_init = []\n",
    "        for X in self.Xs:\n",
    "            X_cpu = X.detach().cpu().numpy()\n",
    "            pca = PCA(n_components=self.output_dims)\n",
    "            Y_pca = pca.fit_transform(X_cpu)\n",
    "            Y_init.append(torch.tensor(Y_pca, device=self.device, dtype=torch.float32))\n",
    "        \n",
    "        Y = torch.stack(Y_init)\n",
    "        Y.requires_grad_()\n",
    "\n",
    "        optimizer = Adam([Y], lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = 0\n",
    "\n",
    "            if epoch < exaggeration_epochs:\n",
    "                P_use = self.Ps * exaggeration\n",
    "            else:\n",
    "                lambd = 0\n",
    "                P_use = self.Ps\n",
    "            \n",
    "            for t in range(T):\n",
    "                Qt, _ = self._compute_lowdim_affinities(Y[t])\n",
    "                loss = self._kl_divergence(P_use[t], Qt)\n",
    "                if t > 0:\n",
    "                    loss += lambd * F.mse_loss(Y[t], Y[t - 1])\n",
    "                total_loss += loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([Y], max_norm=10.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if self.verbose and (epoch % 100 == 0 or epoch == n_epochs - 1):\n",
    "                print(f\"Epoch {epoch}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        return [Y[t].detach().cpu().numpy() for t in range(T)]\n",
    "\n",
    "    def _compute_lowdim_affinities(self, Y):\n",
    "        num = 1 / (1 + torch.cdist(Y, Y, p=2).pow(2))\n",
    "        num.fill_diagonal_(0.0)\n",
    "        Q = torch.clamp(num / num.sum(), min=1e-5)\n",
    "        return Q, num\n",
    "\n",
    "    def _kl_divergence(self, P, Q):\n",
    "        return torch.sum(P * torch.log((P + 1e-8) / (Q + 1e-8)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c133de49-b768-4106-b8da-33358fac4140",
   "metadata": {},
   "source": [
    "subset = [results[\"subset_embeddings\"][i] for i in range(10, len(results[\"subset_embeddings\"]), 30)]\n",
    "len(subset)\n",
    "#samples_per_class = 100\n",
    "#classes = 10  # assuming 1000 samples total and 100 per class\n",
    "#indices = np.concatenate([np.arange(c * 100, c * 100 + samples_per_class) for c in range(classes)])\n",
    "#subset = [emb[:][indices] for emb in subset]\n",
    "#label_subset = [emb[:][indices] for emb in results[\"subset_labels\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "779a3a48-ea63-4147-9070-9a2f42a480d5",
   "metadata": {},
   "source": [
    "tsne = DynamicTSNE()\n",
    "tsne.compute_affinities(subset, perplexity=30.0, k_neighbors=250)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ebaef59e-6776-4280-ae6e-f68dd26a20ad",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "projections = tsne.fit(lr=200, lambd=0.1, n_epochs=1000, exaggeration_epochs=250, exaggeration=22.0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7866586b-984f-4a92-af2d-1a5a09e8d800",
   "metadata": {},
   "source": [
    "%matplotlib widget\n",
    "\n",
    "show_with_slider(\n",
    "    projections,\n",
    "    labels=results[\"subset_labels\"],\n",
    "    interpolate=False,\n",
    "    steps_per_transition=4,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2bad151f-e036-4669-b509-eb62b62501d0",
   "metadata": {},
   "source": [
    "matplotlib.pyplot.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9b29f3b-4b40-4572-8c3d-8bda54d0ce38",
   "metadata": {},
   "source": [
    "visualization_drift_vs_embedding_drift(thesne, embedding_drift_subset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2db8b5a0-5d44-4360-a1ef-326b2278eeca",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Y = tsne_with_live_callback(results[\"subset_embeddings\"][10],\n",
    "                        labels=results[\"subset_labels\"][0],\n",
    "                        perplexity=30,\n",
    "                        lr=200,\n",
    "                        n_iter=10000,\n",
    "                        interval=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "12092497-413c-4512-928b-338aede93667",
   "metadata": {},
   "source": [
    "from openTSNE import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def tsne_with_live_callback(X, labels=None, perplexity=30, lr=200, n_iter=1000, interval=50):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    def callback(iteration, error, Y):\n",
    "        if iteration % interval == 0 or iteration == n_iter - 1:\n",
    "            ax.clear()\n",
    "            if labels is not None:\n",
    "                ax.scatter(Y[:, 0], Y[:, 1], c=labels, cmap='tab10', s=5, alpha=0.7)\n",
    "            else:\n",
    "                ax.scatter(Y[:, 0], Y[:, 1], s=5, alpha=0.7)\n",
    "            ax.set_title(f\"t-SNE at iter {iteration}\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=perplexity,\n",
    "        learning_rate=lr,\n",
    "        n_iter=n_iter,\n",
    "        initialization=\"pca\",\n",
    "        callbacks=callback,\n",
    "        callbacks_every_iters=interval,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        Y = tsne.fit(X)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted, returning current state.\")\n",
    "        return tsne\n",
    "\n",
    "    return Y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "31195665-b273-42a4-808c-a7134f4e0349",
   "metadata": {},
   "source": [
    "# OPENTSNE Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "id": "54e4430f-60ae-42fe-bb8b-5a0ab078719c",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from openTSNE import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "class DynamicTSNE_2:\n",
    "    def __init__(self, perplexity=30, n_iter=1000, init='pca', random_state=None):\n",
    "        \"\"\"\n",
    "        init: 'pca', 'random', or 'previous'\n",
    "        \"\"\"\n",
    "        assert init in ['pca', 'random', 'previous']\n",
    "        self.perplexity = perplexity\n",
    "        self.n_iter = n_iter\n",
    "        self.init = init\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit_transform(self, Xs):\n",
    "        \"\"\"\n",
    "        Xs: List of np.ndarray (each shape: [n_samples, n_features])\n",
    "        Returns: List of np.ndarray (each shape: [n_samples, 2])\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        previous_embedding = None\n",
    "\n",
    "        for i, X in enumerate(Xs):\n",
    "            print(i)\n",
    "            if i == 0 or self.init == 'pca':\n",
    "                init_embedding = PCA(n_components=2).fit_transform(X) if self.init != 'random' else 'random'\n",
    "            else:\n",
    "                init_embedding = previous_embedding\n",
    "\n",
    "            tsne = TSNE(\n",
    "                n_jobs=-1,\n",
    "                perplexity=self.perplexity,\n",
    "                n_iter=self.n_iter,\n",
    "                initialization=init_embedding,\n",
    "                random_state=self.random_state,\n",
    "                verbose=True\n",
    "            )\n",
    "            embedding = tsne.fit(X)\n",
    "            embeddings.append(embedding)\n",
    "            previous_embedding = embedding\n",
    "\n",
    "        return embeddings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9b871ad8-ac78-4c55-b2e9-ad61ab229777",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "dynamic_tsne = DynamicTSNE_2(n_iter=500, init='previous', random_state=42)\n",
    "projections_2 = dynamic_tsne.fit_transform(subset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "060fb98f-d23a-46ab-af8e-75d9f2a4f458",
   "metadata": {},
   "source": [
    "%matplotlib widget\n",
    "\n",
    "show_with_slider(\n",
    "    projections_2,\n",
    "    labels=results[\"subset_labels\"],\n",
    "    interpolate=False,\n",
    "    steps_per_transition=4,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "33e59b0e-c6fb-4a4b-a4a9-9b595fee710b",
   "metadata": {},
   "source": [
    "# MODERN DYNAMIC TNSNE"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4f2bd41-fd84-486a-b33d-894577ccd4b3",
   "metadata": {},
   "source": [
    "subset = [results[\"subset_embeddings\"][i] for i in range(0, len(results[\"subset_embeddings\"]), 5)]\n",
    "samples_per_class = 40\n",
    "classes = 10  # assuming 1000 samples total and 100 per class\n",
    "indices = np.concatenate([np.arange(c * 100, c * 100 + samples_per_class) for c in range(classes)])\n",
    "subset = [emb[:][indices] for emb in subset]\n",
    "label_subset = [emb[:][indices] for emb in results[\"subset_labels\"]]\n",
    "len(subset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dee0d937-b683-4583-b091-9863ea800fc9",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ebbb1297-f353-4fa2-96d7-22342e02c240",
   "metadata": {},
   "source": [
    "modern_dynamic_tsne = ModernDynamicTSNE(\n",
    "    n_epochs=500,\n",
    "    perplexity=30,\n",
    ")\n",
    "projections_3 = modern_dynamic_tsne.fit_transform(subset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5aea256-6526-4d52-91c6-730d9f07b149",
   "metadata": {},
   "source": [
    "show_with_slider(\n",
    "    projections_3,\n",
    "    labels=label_subset,\n",
    "    interpolate=False,\n",
    "    steps_per_transition=4,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2e5e23f-6ac4-4994-9ca5-438e3c98fcea",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "class ModernDynamicTSNE:\n",
    "    def __init__(\n",
    "        self,\n",
    "        perplexity=30,\n",
    "        n_epochs=1000,\n",
    "        output_dims=2,\n",
    "        initial_lr=2400,\n",
    "        final_lr=200,\n",
    "        lr_switch=250,\n",
    "        init_stdev=1e-4,\n",
    "        initial_momentum=0.5,\n",
    "        final_momentum=0.8,\n",
    "        momentum_switch=250,\n",
    "        lmbda=0.0,\n",
    "        sigma_iters=50,\n",
    "        verbose=True,\n",
    "        device=None\n",
    "    ):\n",
    "        self.perplexity = perplexity\n",
    "        self.n_epochs = n_epochs\n",
    "        self.output_dims = output_dims\n",
    "        self.initial_lr = initial_lr\n",
    "        self.final_lr = final_lr\n",
    "        self.lr_switch = lr_switch\n",
    "        self.init_stdev = init_stdev\n",
    "        self.initial_momentum = initial_momentum\n",
    "        self.final_momentum = final_momentum\n",
    "        self.momentum_switch = momentum_switch\n",
    "        self.lmbda = lmbda\n",
    "        self.sigma_iters = sigma_iters\n",
    "        self.verbose = verbose\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _hbeta(self, D, beta):\n",
    "        P = torch.exp(-D * beta)\n",
    "        sumP = torch.sum(P)\n",
    "        sumP = torch.clamp(sumP, min=1e-8)\n",
    "        H = torch.log(sumP) + beta * torch.sum(D * P) / sumP\n",
    "        P = P / sumP\n",
    "        return H, P\n",
    "\n",
    "    def _binary_search_perplexity(self, D, tol=1e-5):\n",
    "        n = D.shape[0]\n",
    "        sigmas = torch.ones(n, device=self.device)\n",
    "        P = torch.zeros((n, n), device=self.device)\n",
    "\n",
    "        logU = np.log(self.perplexity)\n",
    "        for i in range(n):\n",
    "            betamin = None\n",
    "            betamax = None\n",
    "            beta = sigmas[i]\n",
    "            Di = D[i][torch.arange(n) != i]\n",
    "            H, thisP = self._hbeta(Di, beta)\n",
    "\n",
    "            tries = 0\n",
    "            while torch.abs(H - logU) > tol and tries < self.sigma_iters:\n",
    "                if H > logU:\n",
    "                    betamin = beta\n",
    "                    beta = beta * 2 if betamax is None else (beta + betamax) / 2\n",
    "                else:\n",
    "                    betamax = beta\n",
    "                    beta = beta / 2 if betamin is None else (beta + betamin) / 2\n",
    "                H, thisP = self._hbeta(Di, beta)\n",
    "                tries += 1\n",
    "            P[i, torch.arange(n) != i] = thisP\n",
    "        return (P + P.T) / (2 * n)\n",
    "\n",
    "    def _precompute_Ps(self, Xs):\n",
    "        Ps = []\n",
    "        for X in Xs:\n",
    "            D = torch.cdist(X, X).pow(2)\n",
    "            P = self._binary_search_perplexity(D)\n",
    "            Ps.append(P)\n",
    "        return Ps\n",
    "\n",
    "    def _compute_cost(self, Ys, Ps):\n",
    "        total_kl = 0\n",
    "        for Y, P in zip(Ys, Ps):\n",
    "            Q_num = 1 / (1 + torch.cdist(Y, Y).pow(2))\n",
    "            Q_num.fill_diagonal_(0)\n",
    "            Q = Q_num / Q_num.sum()\n",
    "            kl = torch.sum(P * torch.log((P + 1e-8) / (Q + 1e-8)))\n",
    "            total_kl += kl\n",
    "        smoothness = sum((Ys[i] - Ys[i + 1]).pow(2).sum() for i in range(len(Ys) - 1))\n",
    "        return total_kl + self.lmbda * smoothness / (2 * Ys[0].shape[0])\n",
    "\n",
    "    def fit_transform(self, Xs_np):\n",
    "        Xs = [torch.tensor(X, device=self.device, dtype=torch.float32) for X in Xs_np]\n",
    "        T = len(Xs)\n",
    "        N = Xs[0].shape[0]\n",
    "\n",
    "        # Init Ys with PCA\n",
    "        Ys = [\n",
    "            torch.tensor(PCA(n_components=self.output_dims).fit_transform(X.cpu().numpy()),\n",
    "                         device=self.device, dtype=torch.float32, requires_grad=True)\n",
    "            for X in Xs\n",
    "        ]\n",
    "\n",
    "        # Precompute all P matrices once\n",
    "        Ps = self._precompute_Ps(Xs)\n",
    "\n",
    "        optimizer = SGD(Ys, lr=self.initial_lr, momentum=self.initial_momentum)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            if epoch == self.lr_switch:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = self.final_lr\n",
    "            if epoch == self.momentum_switch:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['momentum'] = self.final_momentum\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = self._compute_cost(Ys, Ps)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if self.verbose and (epoch % 100 == 0 or epoch == self.n_epochs - 1):\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        return [Y.detach().cpu().numpy() for Y in Ys]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46fe0592-d668-4594-a1df-2cc8c5873ef2",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
