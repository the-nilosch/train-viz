{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from NeuroVisualizer.neuro_aux.AEmodel import UniformAutoencoder\n",
    "from NeuroVisualizer.neuro_aux.utils import get_files\n",
    "\n",
    "from helper.neuro_viz import get_dataloader_flat\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": [
    "# Neuro-Visualizer\n",
    "This notebook creates the loss landscape from the NeuroVisualizer"
   ],
   "id": "f5326ecff0764644"
  },
  {
   "cell_type": "code",
   "id": "fcbb44e41502b669",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "dataset_name = 'mnist'\n",
    "model_name = 'CNN'\n",
    "runs = [\n",
    "#    \"run-0011-CNN_mnist_32_0.9776\",\n",
    "#    \"run-0012-CNN_mnist_32_0.9768\"\n",
    "#    \"run-0007-CNN_mnist_128_0.9851\",\n",
    "    \n",
    "    # With Residual\n",
    "#    \"run-0016-CNN_cifar10_128_0.8093\", # Seed 42, SAM\n",
    "#    \"run-0018-CNN_cifar10_128_0.8499\", # Seed 42\n",
    "#    \"run-0020-CNN_cifar10_128_0.8079\", # Seed 11, SAM\n",
    "#    \"run-0022-CNN_cifar10_128_0.8519\", # Seed 11\n",
    "    \n",
    "    # No Residual\n",
    "    \"run-0017-CNN_cifar10_128_0.8072\", # Seed 42, SAM\n",
    "    \"run-0019-CNN_cifar10_128_0.8487\", # Seed 42\n",
    "    \"run-0021-CNN_cifar10_128_0.8054\", # Seed 11, SAM\n",
    "    \"run-0023-CNN_cifar10_128_0.8509\", # Seed 11\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helper.data_manager import load_training_data\n",
    "results = []\n",
    "run_ids = []\n",
    "vis_id = \"\"\n",
    "\n",
    "for run in runs:\n",
    "    results.append(load_training_data(run))\n",
    "    run_ids.append(results[-1][\"ll_flattened_weights_dir\"])\n",
    "\n",
    "vis_id = ' x '.join(run_ids)\n",
    "print(run_ids)\n",
    "print(vis_id)"
   ],
   "id": "62e96ade4eb4eb1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6035cac7-16bf-49e4-911d-58b68b9738e3",
   "metadata": {},
   "source": [
    "model_file = f'ae_models/{vis_id}.pt'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f5627ff7e507e37",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Adjust this path to your folder\n",
    "pt_files = []\n",
    "\n",
    "for run_id in run_ids:\n",
    "    model_folder = f\"trainings/{run_id}\"\n",
    "    pt_files.append(get_files(model_folder, prefix=\"model-\"))\n",
    "    print(f\"Found {len(pt_files[-1])} checkpoint files.\")\n",
    "\n",
    "pt_files_flat = [path for sublist in pt_files for path in sublist]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "acb78a0bbfb31f46",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train AE Model\n",
    "Run this part to train an AE-Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "27f3470dc9303199",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "batch_size = 1 #4 - 32 Batch Size of AE Training\n",
    "\n",
    "loader, normalizer = get_dataloader_flat(pt_files_flat, batch_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "id": "da649f617d218fca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Adjust: Choose the hidden dimension (that the model-GPU combination is still working with)"
   ],
   "id": "2c4b8e2cd9f4ae49"
  },
  {
   "cell_type": "code",
   "id": "2a8d44219a8c9983",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "input_dim = loader.dataset[0].shape[0]\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "\n",
    "latent_dim = 2\n",
    "num_layers = 4\n",
    "\n",
    "# Aggressive compression (scales with first hidden dim)\n",
    "#h = [input_dim, 64, 32, 8]\n",
    "h = [input_dim, 128, 64, 16]\n",
    "ae = UniformAutoencoder(input_dim, num_layers, latent_dim, h=h).to(device)\n",
    "\n",
    "#ae = UniformAutoencoder(input_dim, num_layers, latent_dim).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in ae.parameters())\n",
    "trainable_params = sum(p.numel() for p in ae.parameters() if p.requires_grad)\n",
    "\n",
    "size_mb = total_params * 4 / (1024**2)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Approx. size: {size_mb:.2f} MB\")"
   ],
   "id": "a67b3302e2b82a1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42a34c750998a0b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from helper.neuro_viz import train_autoencoder\n",
    "\n",
    "trained_model = train_autoencoder(\n",
    "    model=ae,\n",
    "    train_loader=loader,\n",
    "    device=device,\n",
    "    save_path=model_file,\n",
    "    num_epochs=100,\n",
    "    lr=0.009, # 0.001\n",
    "    patience=15\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98f04cc2b2a5a865",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Visualize Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "id": "570c59ac3e986644",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from NeuroVisualizer.neuro_aux.AEmodel import UniformAutoencoder\n",
    "from NeuroVisualizer.neuro_aux.utils import get_files, repopulate_model\n",
    "from NeuroVisualizer.neuro_aux.trajectories_data import get_trajectory_dataloader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2a9a413afb8e33d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "batch_size = 4\n",
    "loss_name = 'test_loss'\n",
    "whichloss = 'mse' # this is CrossEntropyLoss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20578face5ae73b",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Get file list\n",
    "# pt_files = get_files(model_folder, prefix=\"model-\")\n",
    "\n",
    "# Load AE\n",
    "example_tensor = torch.load(pt_files[0][0], weights_only=True)\n",
    "input_dim = example_tensor.shape[0]\n",
    "latent_dim = 2\n",
    "num_layers = 4\n",
    "#h = [input_dim, 64, 32, 8]\n",
    "h = [input_dim, 128, 64, 16]\n",
    "\n",
    "\n",
    "ae_model = UniformAutoencoder(input_dim, num_layers, latent_dim, h=h).to(device)\n",
    "ae_model.load_state_dict(torch.load(model_file, weights_only=True))\n",
    "#ae_model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9aac66468474164",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ---- Load data ----\n",
    "from helper.neuro_viz import get_dataloader_flat\n",
    "\n",
    "trajectory_loader, transform = get_dataloader_flat(pt_files_flat, batch_size, shuffle=False) #[:5] for Subset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Repopulate original Model Architecture\n",
    "**IMPORTANT: needs correct model**"
   ],
   "id": "361a2b7dae10cfe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for result in results:\n",
    "    print(result[\"model_info\"])"
   ],
   "id": "c66f954b969e6254",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6846d7b6fe6c96a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from helper.vision_classification import init_mlp_for_dataset, init_cnn_for_dataset\n",
    "from helper.neuro_viz import Loss\n",
    "\n",
    "#TODO Check the model:\n",
    "model = init_cnn_for_dataset(dataset_name, conv_dims=[8, 16], kernel_sizes=[3, 3], hidden_dims=[32], dropout=0.25, residual=False).to(device)\n",
    "#model = init_cnn_for_dataset(dataset_name, conv_dims=[8, 16], kernel_sizes=[3, 3], hidden_dims=[32], dropout=0.25, residual=True).to(device)\n",
    "\n",
    "#model = init_cnn_for_dataset(dataset_name, conv_dims=[32, 64], kernel_sizes=[3, 3], hidden_dims=[128], dropout=0.25, residual=False).to(device)\n",
    "#model = init_cnn_for_dataset(dataset, conv_dims=[32, 64], kernel_sizes=[3, 3], hidden_dims=[128], dropout=0.25, residual=True).to(device)\n",
    "\n",
    "#model = init_cnn_for_dataset(dataset_name, conv_dims=[64, 128, 256], kernel_sizes=[5, 3, 3], hidden_dims=[256, 128], dropout=0.2, residual=True).to(device)\n",
    "#model = init_mlp_for_dataset(dataset_name, hidden_dims=[254, 64], dropout=0.1).to(device)\n",
    "loss_obj = Loss(dataset_name, device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Compute trajectory (Coordinates and Loss)"
   ],
   "id": "f2236a9451729ed6"
  },
  {
   "cell_type": "code",
   "id": "a264ebc0a37b88af",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from helper.neuro_viz import compute_trajectory\n",
    "\n",
    "trajectory_coordinates, trajectory_models, trajectory_losses = compute_trajectory(\n",
    "    trajectory_loader,\n",
    "    ae_model,\n",
    "    transform,\n",
    "    loss_obj,\n",
    "    model,\n",
    "    loss_name,\n",
    "    whichloss,\n",
    "    device,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_chunks = len(pt_files)\n",
    "chunk_size = len(pt_files[0])\n",
    "\n",
    "# Reshape the values\n",
    "tr_losses = np.split(trajectory_losses.cpu().numpy(), num_chunks)  # list of arrays\n",
    "tr_coordinates = np.split(trajectory_coordinates.cpu().numpy(), num_chunks)  # list of arrays"
   ],
   "id": "36a3d3e11546ffc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a432896303748a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    plt.plot(results[i][\"val_losses\"], label='Logged Validation Loss', marker='o')\n",
    "    plt.plot(reshaped[i], label='AE-Projected Validation Loss', marker='x')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Validation Loss: Training Log vs. AE-Projected Trajectory')\n",
    "    plt.xlabel('Checkpoint Index')\n",
    "    plt.ylabel('Loss (Cross Entropy)')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e322ad33c28b9b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generate grid in latent space\n",
    "from helper.neuro_viz import generate_latent_grid, compute_grid_losses\n",
    "xx, yy, grid_coords = generate_latent_grid(\n",
    "    min_map=-1, max_map=1,\n",
    "    xnum=10, # 3 - 25\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Decode grid and compute losses\n",
    "#model = init_mlp_for_dataset(dataset_name, hidden_dims=[254, 64], dropout=0.1).to(device)\n",
    "\n",
    "grid_losses = compute_grid_losses(\n",
    "    grid_coords,\n",
    "    transform,\n",
    "    ae_model,\n",
    "    model,\n",
    "    loss_obj,\n",
    "    loss_name,\n",
    "    whichloss,\n",
    "    device\n",
    ")\n",
    "\n",
    "# Reshape to grid\n",
    "grid_losses = grid_losses.view(xx.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89ebf45f637a2183",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(grid_losses.min().item(), grid_losses.max().item())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbe82af7fb974760",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rec_grid_models = ae_model.decoder(grid_coords)\n",
    "rec_grid_models = rec_grid_models*transform.std.to(device) + transform.mean.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helper.neuro_viz import plot_loss_landscape\n",
    "\n",
    "fig = plot_loss_landscape(\n",
    "    xx, yy,\n",
    "    grid_losses,\n",
    "    tr_losses,\n",
    "    tr_coordinates,\n",
    "    rec_grid_models=rec_grid_models,\n",
    "    draw_density=False,\n",
    "    filled_contours=False\n",
    ")"
   ],
   "id": "a610fae84338aaf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f9eb868b9361be4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Save to PDF\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "fig.savefig(f'plots/loss_landscape_{vis_id}.pdf', dpi=300, bbox_inches='tight', format='pdf')\n",
    "print(f\"Saved PDF to plots/loss_landscape_{vis_id}.pdf\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [],
   "id": "36aa8683dde74e62",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (train-viz)",
   "language": "python",
   "name": "train-viz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
