{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "from NeuroVisualizer.neuro_aux.AEmodel import UniformAutoencoder\n",
    "\n",
    "from helper.neuro_viz import get_dataloader_flat\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": [
    "# Neuro-Visualizer\n",
    "This notebook creates the loss landscape from the NeuroVisualizer"
   ],
   "id": "f5326ecff0764644"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_name = 'mnist'\n",
    "run_ids = [\n",
    "    \"run-0011-CNN_mnist_32_0.9776\", # No Residual\n",
    "    \"run-0012-CNN_mnist_32_0.9768\", # No Residual\n",
    "]\n",
    "\n",
    "titles = [\n",
    "    \"SAM, 0.9776\",\n",
    "    \"SGD, 0.9768\"\n",
    "]"
   ],
   "id": "58929cbf76aab1ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# CNN x CIFAR 10 \n",
    "dataset_name = 'cifar10'\n",
    "\n",
    "run_ids = [\n",
    "    \"run-0017-CNN_cifar10_128_0.8072\",  # Seed 42, SAM\n",
    "    \"run-0019-CNN_cifar10_128_0.8487\",  # Seed 42\n",
    "    \"run-0021-CNN_cifar10_128_0.8054\",  # Seed 11, SAM\n",
    "    \"run-0023-CNN_cifar10_128_0.8509\",  # Seed 11\n",
    "    \"run-0025-CNN_cifar10_128_0.8062\",\n",
    "    \"run-0027-CNN_cifar10_128_0.8503\"\n",
    "]\n",
    "\n",
    "titles = [\n",
    "    \"Seed 42, SAM, 0.8072\",\n",
    "    \"Seed 42, SGD, 0.8487\",\n",
    "    \"Seed 11, SAM, 0.8054\",\n",
    "    \"Seed 11, SGD, 0.8509\",\n",
    "    \"Seed 6, SAM, 0.8062\",\n",
    "    \"Seed 6, SGD, 0.8503\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddcb593552b6efba",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# CNN Residual x CIFAR 10 \n",
    "dataset_name = 'cifar10'\n",
    "\n",
    "run_ids = [\n",
    "    \"run-0016-CNN_cifar10_128_0.8093\",  # Seed 42, SAM, Residual\n",
    "    \"run-0018-CNN_cifar10_128_0.8499\",  # Seed 42, Residual\n",
    "    \"run-0020-CNN_cifar10_128_0.8079\",  # Seed 11, SAM, Residual\n",
    "    \"run-0022-CNN_cifar10_128_0.8519\",  # Seed 11, Residual\n",
    "    \"run-0024-CNN_cifar10_128_0.8062\",\n",
    "    \"run-0026-CNN_cifar10_128_0.8504\"\n",
    "]\n",
    "\n",
    "titles = [\n",
    "    \"Seed 42, SAM, Residual 0.8093\",\n",
    "    \"Seed 42, SGD, Residual 0.8499\",\n",
    "    \"Seed 11, SAM, Residual 0.8079\",\n",
    "    \"Seed 11, SGD, Residual 0.8519\",\n",
    "    \"Seed 6, SAM, Residual 0.0.8062\",\n",
    "    \"Seed 6, SGD, Residual 0.8504\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d874059863863f",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load Paths"
   ],
   "id": "b394f02155933341"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helper.visualization import Run\n",
    "\n",
    "runs = []\n",
    "for run_id in run_ids:\n",
    "    runs.append(Run(run_id, dataset_name))"
   ],
   "id": "a2b5ff08c91fbd4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pt_files_per_run = [run.get_pt_files() for run in runs]"
   ],
   "id": "d9b1939826759578",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vis_id = ' x '.join([run.results[\"ll_flattened_weights_dir\"] for run in runs])\n",
    "model_file = f'ae_models/{vis_id}.pt'\n",
    "print(model_file)\n",
    "\n",
    "#model_file = \"ae_models/run-0016-CNN x run-0018-CNN x run-0020-CNN x run-0022-CNN x run-0024-CNN x run-0026-CNN.pt\""
   ],
   "id": "59a509a266a496e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pt_files_per_run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a434756e0bb87",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Filter for final epochs only\n",
    "\n",
    "pt_files_per_run = []\n",
    "\n",
    "for run in runs:\n",
    "    min_loss = min(run.results[\"val_losses\"])\n",
    "    max_visualize = min_loss * 1.1\n",
    "    print(max_visualize)\n",
    "    # Find in run.results[\"val_losses\"] idx where val losses is first below max_visualize\n",
    "    idx = next((i for i, v in enumerate(run.results[\"val_losses\"]) if v <= max_visualize), None)\n",
    "    pt_files_per_run.append(run.get_pt_files()[idx:])\n",
    "    \n",
    "pt_files_per_run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ab527424950edef",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "acb78a0bbfb31f46",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train AE Model\n",
    "Run this part to train an AE-Model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 3 #4 - 32 Batch Size of AE Training\n",
    "\n",
    "loader, normalizer = get_dataloader_flat(\n",
    "    pt_files_per_run,\n",
    "    batch_size,\n",
    "    include_lmc=False,\n",
    "    shuffle=True,\n",
    "    oversample_later=False, # more samples from later epochs, that diverge more\n",
    "    power=1.0\n",
    ")"
   ],
   "id": "9a18114eaaaab3a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "id": "c5dafee49f121593",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Adjust: Choose the hidden dimension (that the model-GPU combination is still working with)"
   ],
   "id": "2c4b8e2cd9f4ae49"
  },
  {
   "cell_type": "code",
   "id": "2a8d44219a8c9983",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "input_dim = loader.dataset[0].shape[0]\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "\n",
    "latent_dim = 2\n",
    "num_layers = 4\n",
    "\n",
    "# Aggressive compression (scales with first hidden dim)\n",
    "#h = [input_dim, 64, 32, 8]\n",
    "#h = [input_dim, 126, 64, 32]\n",
    "#h = [input_dim, 200, 100, 50]\n",
    "#ae = UniformAutoencoder(input_dim, num_layers, latent_dim, h=h).to(device)\n",
    "\n",
    "ae = UniformAutoencoder(input_dim, num_layers, latent_dim).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in ae.parameters())\n",
    "trainable_params = sum(p.numel() for p in ae.parameters() if p.requires_grad)\n",
    "\n",
    "size_mb = total_params * 4 / (1024**2)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Approx. size: {size_mb:.2f} MB\")\n",
    "\n",
    "# print(f\"Approx. size: {size_mb:.2f} MB\")"
   ],
   "id": "a67b3302e2b82a1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load from previous train process (if available, eg. after a crash)\n",
    "ae.load_state_dict(torch.load(model_file, weights_only=True))"
   ],
   "id": "839c74395255eceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helper.neuro_viz import train_autoencoder\n",
    "\n",
    "trained_model = train_autoencoder(\n",
    "    model=ae,\n",
    "    train_loader=loader,\n",
    "    device=device,\n",
    "    save_path=model_file,\n",
    "    num_epochs=100, #1000 would be great\n",
    "    lr=0.0001, # Start with 0.01\n",
    "    patience=15,\n",
    "    avoid_overheat=False, # Avoids chrashes on Nembus Computer\n",
    "    last_saved_loss=0.40143, # Minimum Loss to save\n",
    "    save_delta_pct=0.02,\n",
    "    verbose=True\n",
    ")\n",
    "# ~ 0.0173 possible (CIFAR10 CNN)"
   ],
   "id": "6304411380a6b2a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.save(ae.state_dict(), model_file)"
   ],
   "id": "5f12cb3ff59f854b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98f04cc2b2a5a865",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Visualize Trajectory\n",
    "Begin here when trained Autoencoder (AE) can be loaded"
   ]
  },
  {
   "cell_type": "code",
   "id": "570c59ac3e986644",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from NeuroVisualizer.neuro_aux.AEmodel import UniformAutoencoder\n",
    "from NeuroVisualizer.neuro_aux.utils import get_files, repopulate_model\n",
    "from NeuroVisualizer.neuro_aux.trajectories_data import get_trajectory_dataloader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2a9a413afb8e33d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "batch_size = 4\n",
    "loss_name = 'test_loss'\n",
    "whichloss = 'mse' # this is CrossEntropyLoss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20578face5ae73b",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Get file list\n",
    "# pt_files = get_files(model_folder, prefix=\"model-\")\n",
    "\n",
    "# Load AE\n",
    "example_tensor = torch.load(pt_files_per_run[0][0], weights_only=True)\n",
    "input_dim = example_tensor.shape[0]\n",
    "latent_dim = 2\n",
    "num_layers = 4\n",
    "#h = [input_dim, 64, 32, 8]\n",
    "#h = [input_dim, 128, 64, 16]\n",
    "#h = [input_dim, 200, 100, 50]\n",
    "\n",
    "#ae_model = UniformAutoencoder(input_dim, num_layers, latent_dim, h=h).to(device)\n",
    "ae_model = UniformAutoencoder(input_dim, num_layers, latent_dim).to(device)\n",
    "ae_model.load_state_dict(torch.load(model_file, weights_only=True))\n",
    "_ = ae_model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9aac66468474164",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ---- Load data ----\n",
    "from helper.neuro_viz import get_dataloader_flat\n",
    "\n",
    "trajectory_loader, transform = get_dataloader_flat(pt_files_per_run, batch_size, shuffle=False) #[:5] for Subset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Repopulate original Model Architecture\n",
    "**IMPORTANT: needs correct model**"
   ],
   "id": "361a2b7dae10cfe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for run in runs:\n",
    "    print(run.results[\"model_info\"])"
   ],
   "id": "c66f954b969e6254",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helper.vision_classification import init_mlp_for_dataset, init_cnn_for_dataset\n",
    "from helper.neuro_viz import Loss\n",
    "\n",
    "#TODO Check the model:\n",
    "model = init_cnn_for_dataset(dataset_name, conv_dims=[8, 16], kernel_sizes=[3, 3], hidden_dims=[32], dropout=0.25, residual=False).to(device)\n",
    "#model = init_cnn_for_dataset(dataset_name, conv_dims=[8, 16], kernel_sizes=[3, 3], hidden_dims=[32], dropout=0.25, residual=True).to(device)\n",
    "\n",
    "#model = init_cnn_for_dataset(dataset_name, conv_dims=[32, 64], kernel_sizes=[3, 3], hidden_dims=[128], dropout=0.25, residual=False).to(device)\n",
    "#model = init_cnn_for_dataset(dataset, conv_dims=[32, 64], kernel_sizes=[3, 3], hidden_dims=[128], dropout=0.25, residual=True).to(device)\n",
    "#model = init_cnn_for_dataset(dataset_name, conv_dims=[64, 128, 256], kernel_sizes=[5, 3, 3], hidden_dims=[256, 128], dropout=0.2, residual=True).to(device)\n",
    "#model = init_mlp_for_dataset(dataset_name, hidden_dims=[254, 64], dropout=0.1).to(device)\n",
    "loss_obj = Loss(dataset_name, device)"
   ],
   "id": "d6c5003998a319ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Trainable parameters: {count_parameters(model):,}\")\n",
    "print(f\"AE input/output dim:  {ae_model.encoder.fcs[0].in_features:,}\")"
   ],
   "id": "2f1206dc0e81cc0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Compute trajectory (Coordinates and Loss)"
   ],
   "id": "eff1d929fb6993e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helper.neuro_viz import compute_trajectory\n",
    "\n",
    "trajectory_coordinates, trajectory_models, trajectory_losses = compute_trajectory(\n",
    "    trajectory_loader,\n",
    "    ae_model,\n",
    "    transform,\n",
    "    loss_obj,\n",
    "    model,\n",
    "    loss_name,\n",
    "    whichloss,\n",
    "    device,\n",
    "    recalibrate_bn=True, # Optimizes Loss precision\n",
    ")"
   ],
   "id": "7705dad961771adb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from tqdm import tqdm\n",
    "from helper.neuro_viz import repopulate_model_fixed\n",
    "\n",
    "def _flatten_model_vec(m: torch.nn.Module) -> torch.Tensor:\n",
    "    # If you have a project-specific flattener, call it here instead.\n",
    "    return parameters_to_vector(m.parameters()).detach()\n",
    "\n",
    "def compute_trajectory(\n",
    "    trajectory_loader,\n",
    "    ae_model,\n",
    "    transform,\n",
    "    loss_obj,\n",
    "    model,\n",
    "    loss_name,\n",
    "    whichloss,\n",
    "    device,\n",
    "    recalibrate_bn: bool = True,\n",
    "    bn_recal_batches: int = 100,\n",
    "    bn_loader=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      trajectory_coordinates: [N, 2]\n",
    "      trajectory_models:     [N, D] (decoded, de-normalized)\n",
    "      trajectory_losses:     [N]    (task loss per repopulated model)\n",
    "      ae_losses_decode:      [N]    (AE loss on normalized input -> output during decode)\n",
    "      ae_losses_finetuned:   [N]    (AE loss after repopulate/BN on flattened model)\n",
    "    \"\"\"\n",
    "    if not recalibrate_bn:\n",
    "        print(\"Tip: set recalibrate_bn=True to refresh BN stats for more accurate losses.\")\n",
    "\n",
    "    ae_model.eval()\n",
    "    model = model.to(device)\n",
    "    model_device = next(model.parameters()).device\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    mean = transform.mean.to(device)\n",
    "    std  = transform.std.to(device)\n",
    "\n",
    "    # ---- Decode trajectory + AE loss (no fine-tuning) ----\n",
    "    trajectory_models, trajectory_coordinates = [], []\n",
    "    ae_losses_decode = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(trajectory_loader, desc=\"Decoding trajectory\"):\n",
    "            # batch: normalized flattened weights, shape [B, D]\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            x_recon_norm, z = ae_model(batch)                 # AE output in normalized space\n",
    "            # per-sample MSE over feature dim\n",
    "            ae_mse = F.mse_loss(x_recon_norm, batch, reduction='none').mean(dim=1)  # [B]\n",
    "            ae_losses_decode.append(ae_mse.cpu())\n",
    "\n",
    "            # store coords + de-normalized decoded weights for repopulation\n",
    "            trajectory_coordinates.append(z.cpu())\n",
    "            x_recon = x_recon_norm * std + mean               # de-normalize\n",
    "            trajectory_models.append(x_recon.cpu())\n",
    "\n",
    "    trajectory_coordinates = torch.cat(trajectory_coordinates, dim=0)\n",
    "    trajectory_models = torch.cat(trajectory_models, dim=0)\n",
    "    ae_losses_decode = torch.cat(ae_losses_decode, dim=0).float()  # [N]\n",
    "\n",
    "    print(f\"✅ Decoded trajectory shapes: coords {trajectory_coordinates.shape}, models {trajectory_models.shape}\")\n",
    "\n",
    "    # BN recal source\n",
    "    bn_src = bn_loader or getattr(loss_obj, \"train_loader\", None) or trajectory_loader\n",
    "\n",
    "    # ---- Compute task losses + AE loss after repopulation ----\n",
    "    trajectory_losses = []\n",
    "    ae_losses_finetuned = []\n",
    "\n",
    "    for i in tqdm(range(trajectory_models.shape[0]), desc=\"Computing trajectory & AE(finetuned) losses\"):\n",
    "        flat_cpu = trajectory_models[i, :]\n",
    "        assert flat_cpu.numel() == total_params, \"Mismatch in parameter size.\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # repopulate model from decoded weights\n",
    "            flat = flat_cpu.to(model_device, non_blocking=True)\n",
    "            model = repopulate_model_fixed(flat, model)  # in-place or returns model\n",
    "\n",
    "        # (optional) BN recalibration\n",
    "        if recalibrate_bn and bn_src is not None:\n",
    "            model.train()\n",
    "            with torch.no_grad():\n",
    "                for b_idx, batch in enumerate(bn_src):\n",
    "                    x = batch[0] if (isinstance(batch, (list, tuple)) and len(batch) >= 1) else batch\n",
    "                    model(x.to(model_device, non_blocking=True))\n",
    "                    if b_idx + 1 >= bn_recal_batches:\n",
    "                        break\n",
    "            model.eval()\n",
    "\n",
    "        # Task loss\n",
    "        with torch.no_grad():\n",
    "            loss_val = loss_obj.get_loss(model, loss_name, whichloss).item()\n",
    "        trajectory_losses.append(loss_val)\n",
    "\n",
    "        # AE loss on the (re)flattened, finetuned model\n",
    "        with torch.no_grad():\n",
    "            flat_ft = _flatten_model_vec(model).to(device).view(1, -1)   # [1, D] on AE device\n",
    "            flat_ft_norm = (flat_ft - mean) / std                        # normalize to AE space\n",
    "            recon_ft_norm, _ = ae_model(flat_ft_norm)                    # AE forward\n",
    "            ae_mse_ft = F.mse_loss(recon_ft_norm, flat_ft_norm, reduction='none').mean(dim=1)  # [1]\n",
    "            ae_losses_finetuned.append(ae_mse_ft.squeeze(0).cpu().item())\n",
    "\n",
    "    trajectory_losses = torch.tensor(trajectory_losses, dtype=torch.float32)         # [N]\n",
    "    ae_losses_finetuned = torch.tensor(ae_losses_finetuned, dtype=torch.float32)     # [N]\n",
    "\n",
    "    print(f\"✅ Computed {trajectory_losses.shape[0]} task losses\\n\"\n",
    "          f\"AE(decode) losses: {ae_losses_decode.mean()}, \"\n",
    "          f\"AE(finetuned) losses: {ae_losses_finetuned.mean()}\")\n",
    "\n",
    "    return trajectory_coordinates, trajectory_models, trajectory_losses\n",
    "\n",
    "trajectory_coordinates, trajectory_models, trajectory_losses = compute_trajectory(\n",
    "    trajectory_loader,\n",
    "    ae_model,\n",
    "    transform,\n",
    "    loss_obj,\n",
    "    model,\n",
    "    loss_name,\n",
    "    whichloss,\n",
    "    device,\n",
    "    recalibrate_bn=True, # Optimizes Loss precision\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa52b9443ea9a79e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(ae_losses_decode.mean())\n",
    "print(ae_losses_finetuned.mean())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d183ecb3cbfe678c",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get lengths for each run\n",
    "chunk_sizes = [len(run) for run in pt_files_per_run]\n",
    "num_chunks = len(chunk_sizes)\n",
    "\n",
    "# Split trajectory arrays according to these lengths\n",
    "tr_losses = np.split(trajectory_losses.cpu().numpy(), np.cumsum(chunk_sizes)[:-1])\n",
    "tr_coordinates = np.split(trajectory_coordinates.cpu().numpy(), np.cumsum(chunk_sizes)[:-1])"
   ],
   "id": "36a3d3e11546ffc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "real_losses = [run.results[\"val_losses\"] for run in runs]"
   ],
   "id": "3234ec1bd60282af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fix, that there is one epoch 0 for the pt files\n",
    "for i in range(num_chunks):\n",
    "    #first_loss = tr_losses[i][0]\n",
    "    real_losses[i] = np.concatenate(([np.NaN], real_losses[i]))"
   ],
   "id": "fb7d461733d0a60a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "titles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c3becfa2ae648df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a432896303748a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = 2\n",
    "rows = int(np.ceil(num_chunks / cols))\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(14, rows * 4), squeeze=False)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    r, c = divmod(i, cols)\n",
    "    ax = axes[r, c]\n",
    "\n",
    "    ax.plot(real_losses[i], label='Logged Validation Loss', marker='o')\n",
    "    ax.plot(tr_losses[i], label='AE-Projected Validation Loss', marker='x')\n",
    "\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel('Checkpoint Index')\n",
    "    ax.set_ylabel('Loss (Cross Entropy)')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "# Hide unused subplots (if odd number of runs)\n",
    "for j in range(num_chunks, rows * cols):\n",
    "    r, c = divmod(j, cols)\n",
    "    fig.delaxes(axes[r, c])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate grid in latent space\n",
    "from helper.neuro_viz import generate_latent_grid, compute_grid_losses, compute_grid_losses_batched\n",
    "xx, yy, grid_coords = generate_latent_grid(\n",
    "    min_map=-1.1, max_map=1.1,\n",
    "    xnum=10, # 3 - 25\n",
    "    device=device\n",
    ")\n",
    "\n",
    "grid_losses = compute_grid_losses_batched(\n",
    "    grid_coords,\n",
    "    transform,\n",
    "    ae_model,\n",
    "    model,\n",
    "    loss_obj,\n",
    "    loss_name,\n",
    "    whichloss,\n",
    "    device,\n",
    "    bn_recal_batches=30,\n",
    ")\n",
    "\n",
    "# Reshape to grid\n",
    "grid_losses = grid_losses.view(xx.shape)"
   ],
   "id": "b88285395fb06cb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(grid_losses.min().item(), grid_losses.max().item())"
   ],
   "id": "ae5c01aa998f81a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbe82af7fb974760",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rec_grid_models = ae_model.decoder(grid_coords)\n",
    "rec_grid_models = rec_grid_models*transform.std.to(device) + transform.mean.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If CUDA out of memory"
   ],
   "id": "e0857fbb5674fb15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def decode_grid_in_batches(ae_model, grid_coords, transform, device, batch_size=32):\n",
    "    ae_model.eval()\n",
    "    std = transform.std.to(device)\n",
    "    mean = transform.mean.to(device)\n",
    "    chunks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, grid_coords.size(0), batch_size):\n",
    "            coords = grid_coords[i : i + batch_size].to(device)      # [B,2]\n",
    "            rec = ae_model.decoder(coords)                            # [B, D]\n",
    "            rec = rec * std + mean                                    # [B, D]\n",
    "            chunks.append(rec.cpu())     # move back to CPU immediately\n",
    "            del coords, rec\n",
    "            torch.cuda.empty_cache()     # free any cached GPU memory\n",
    "\n",
    "    return torch.cat(chunks, dim=0)      # [N, D]"
   ],
   "id": "e058707b8de26330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rec_grid_models = decode_grid_in_batches(\n",
    "    ae_model, grid_coords, transform, device, batch_size=16\n",
    ")"
   ],
   "id": "902678d7b7d136be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from helper.neuro_viz import plot_loss_landscape\n",
    "\n",
    "fig = plot_loss_landscape(\n",
    "    xx, yy,\n",
    "    grid_losses,\n",
    "    real_losses, # real_losses or tr_losses\n",
    "    tr_coordinates,\n",
    "    rec_grid_models=rec_grid_models,\n",
    "    draw_density=False,\n",
    "    filled_contours=False\n",
    ")"
   ],
   "id": "a610fae84338aaf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f9eb868b9361be4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Save to PDF\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "fig.savefig(f'plots/loss_landscape_{vis_id}.pdf', dpi=300, bbox_inches='tight', format='pdf')\n",
    "print(f\"Saved PDF to plots/loss_landscape_{vis_id}.pdf\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plot_loss_landscape(\n",
    "    xx, yy,\n",
    "    grid_losses,\n",
    "    real_losses, # real_losses or tr_losses\n",
    "    tr_coordinates,\n",
    "    rec_grid_models=rec_grid_models,\n",
    "    draw_density=False,\n",
    "    filled_contours=False,\n",
    "    trajectory_labels=titles,            # NEW: list of strs, one per trajectory - ['Test 1', 'Test 2', 'Test 3', 'Test 4'],\n",
    "    label_positions=[('left', 'top'), ('left', 'bottom'), ('right', 'bottom'), ('right', 'top'), ('left', 'center'), ('center', 'bottom')], # ('left'|'center'|'right', 'top'|'center'|'bottom')\n",
    ")"
   ],
   "id": "a238c0a7132f3419",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def plot_loss_landscape(\n",
    "    xx, yy,\n",
    "    grid_losses, trajectory_losses_list, trajectory_coords_list,\n",
    "    rec_grid_models=None,\n",
    "    draw_density=True,\n",
    "    filled_contours=True,\n",
    "    cmap='viridis',\n",
    "    loss_label='Cross Entropy Loss',\n",
    "    trajectory_labels=None,\n",
    "    label_positions=None,\n",
    "):\n",
    "    # === PREPARE LOSSES ===\n",
    "    grid_losses_pos = grid_losses.detach().cpu().numpy()\n",
    "\n",
    "    # === SHARED COLOR SCALE ===\n",
    "    traj_losses_all = np.concatenate([t for t in trajectory_losses_list])\n",
    "    all_losses = np.concatenate([grid_losses_pos.flatten(), traj_losses_all])\n",
    "    vmin = np.clip(all_losses.min() / 1.2, 1e-5, None)\n",
    "    vmax = all_losses.max() * 1.2\n",
    "\n",
    "    if vmin >= vmax or np.isclose(vmin, vmax):\n",
    "        vmax = vmin * 10\n",
    "        print(f\"Adjusted nearly-constant losses: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "    levels = np.logspace(np.log10(vmin), np.log10(vmax), 30)\n",
    "    norm = LogNorm(vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # === BEGIN PLOTTING ===\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # -- 1 Loss Landscape --\n",
    "    X = xx.cpu().numpy()\n",
    "    Y = yy.cpu().numpy()\n",
    "\n",
    "    if filled_contours:\n",
    "        contour = ax.contourf(X, Y, grid_losses_pos, levels=levels, norm=norm, cmap=cmap)\n",
    "    else:\n",
    "        contour = ax.contour(X, Y, grid_losses_pos, levels=levels, norm=norm, cmap=cmap)\n",
    "        ax.clabel(contour, fmt=\"%.2e\", fontsize=8)\n",
    "\n",
    "    cbar = plt.colorbar(contour, ax=ax, shrink=0.8)\n",
    "    ticks = np.logspace(np.log10(vmin), np.log10(vmax), 5)  # customize number here\n",
    "    cbar.set_ticks(ticks)\n",
    "    cbar.ax.set_ylabel(loss_label, fontsize=12)\n",
    "\n",
    "    # -- 2 & 3: Plot Multiple Trajectories --\n",
    "    for z_tensor, losses_tensor in zip(trajectory_coords_list, trajectory_losses_list):\n",
    "        z = z_tensor\n",
    "        losses = losses_tensor\n",
    "        # Lines\n",
    "        for i in range(len(z) - 1):\n",
    "            ax.plot([z[i, 0], z[i + 1, 0]], [z[i, 1], z[i + 1, 1]], color='k', linewidth=1)\n",
    "        # Points\n",
    "        ax.scatter(\n",
    "            z[:, 0], z[:, 1],\n",
    "            c=losses,\n",
    "            cmap=cmap,\n",
    "            norm=norm,\n",
    "            s=40,\n",
    "            edgecolors='k'\n",
    "        )\n",
    "\n",
    "    # ===== 3b: Annotate each trajectory at its last point =====\n",
    "    offset_pts = 5  # how far, in points, to shift the label\n",
    "\n",
    "    # defaults\n",
    "    n_traj = len(trajectory_coords_list)\n",
    "    if trajectory_labels is None:\n",
    "        trajectory_labels = [f\"traj {i}\" for i in range(n_traj)]\n",
    "    if label_positions is None:\n",
    "        label_positions = ['auto'] * n_traj\n",
    "\n",
    "    for idx, (z, losses, lab) in enumerate(zip(\n",
    "            trajectory_coords_list,\n",
    "            trajectory_losses_list,\n",
    "            trajectory_labels)):\n",
    "        x_end, y_end = float(z[-1, 0]), float(z[-1, 1])\n",
    "\n",
    "        # decide alignment\n",
    "        pos = label_positions[idx]\n",
    "        if pos != 'auto':\n",
    "            ha, va = pos\n",
    "        else:\n",
    "            dx = z[-1, 0] - z[-2, 0]\n",
    "            dy = z[-1, 1] - z[-2, 1]\n",
    "            ha = 'left'   if dx >= 0 else 'right'\n",
    "            va = 'bottom' if dy >= 0 else 'top'\n",
    "\n",
    "        # convert alignment into point‐offset direction\n",
    "        ox =  offset_pts if ha == 'left'   else (-offset_pts if ha == 'right' else 0)\n",
    "        oy =  offset_pts if va == 'bottom' else (-offset_pts if va == 'top'   else 0)\n",
    "\n",
    "        # annotate with offset\n",
    "        ax.annotate(\n",
    "            lab,\n",
    "            xy=(x_end, y_end),\n",
    "            xytext=(ox, oy),\n",
    "            textcoords='offset points',\n",
    "            ha=ha, va=va,\n",
    "            fontsize=7,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", alpha=0.6),\n",
    "            arrowprops=dict(arrowstyle='-', lw=0)\n",
    "        )\n",
    "\n",
    "    # -- 4 OPTIONAL: Density Contours --\n",
    "    if draw_density and rec_grid_models is not None:\n",
    "        try:\n",
    "            from NeuroVisualizer.neuro_aux.utils import get_density\n",
    "            density = get_density(rec_grid_models.detach().cpu().numpy(), type='inverse', p=2)\n",
    "            density = density.reshape(xx.shape)\n",
    "            density_levels = np.logspace(\n",
    "                np.log10(max(density.min(), 1e-3)),\n",
    "                np.log10(density.max()),\n",
    "                15\n",
    "            )\n",
    "            CS_density = ax.contour(\n",
    "                X, Y, density,\n",
    "                levels=density_levels,\n",
    "                colors='white',\n",
    "                linewidths=0.8\n",
    "            )\n",
    "            ax.clabel(CS_density, fmt=ticker.FormatStrFormatter('%.1f'), fontsize=7)\n",
    "        except Exception as e:\n",
    "            print(\"Density contour skipped:\", e)\n",
    "\n",
    "    # -- 5 Labels, Grid, Style --\n",
    "    ax.set_title('Loss Landscape with Training Trajectory', fontsize=14)\n",
    "    ax.set_xlabel('Latent Dimension 1', fontsize=12)\n",
    "    ax.set_ylabel('Latent Dimension 2', fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "    # -- 6 Show --\n",
    "    #plt.show()\n",
    "\n",
    "    return fig"
   ],
   "id": "36aa8683dde74e62",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (phate-env)",
   "language": "python",
   "name": "phate-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
