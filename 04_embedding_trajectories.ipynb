{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f02a8-99fe-4426-95b6-5ef290a7ad48",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from helper.plots import plot_phate_animations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad5593de73e53a",
   "metadata": {},
   "source": [
    "# Embedding Trajectories\n",
    "Compare embedding trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'mnist'\n",
    "\n",
    "run_ids = [\n",
    "#    \"run-0011-CNN_mnist_32_0.9776\",\n",
    "#    \"run-0012-CNN_mnist_32_0.9768\"\n",
    "#    \"run-0007-CNN_mnist_128_0.9851\",\n",
    "\n",
    "#    \"run-0016-CNN_cifar10_128_0.8093\", # Seed 42, SAM, Residual\n",
    "    \"run-0017-CNN_cifar10_128_0.8072\", # Seed 42, SAM\n",
    "#    \"run-0018-CNN_cifar10_128_0.8499\", # Seed 42, Residual\n",
    "    \"run-0019-CNN_cifar10_128_0.8487\", # Seed 42\n",
    "\n",
    "#    \"run-0020-CNN_cifar10_128_0.8079\", # Seed 11, SAM, Residual\n",
    "    \"run-0021-CNN_cifar10_128_0.8054\", # Seed 11, SAM\n",
    "#    \"run-0022-CNN_cifar10_128_0.8519\", # Seed 11, Residual\n",
    "    \"run-0023-CNN_cifar10_128_0.8509\", # Seed 11\n",
    "\n",
    "#    \"run-0024-CNN_cifar10_128_0.8062\",\n",
    "    \"run-0025-CNN_cifar10_128_0.8062\",\n",
    "#    \"run-0026-CNN_cifar10_128_0.8504\",\n",
    "    \"run-0027-CNN_cifar10_128_0.8503\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa161133-7cfe-4c71-84b2-5c7b3f958f41",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "titles = [\n",
    "#    \"Seed 42, SAM, Residual 0.8093\",\n",
    "    \"Seed 42, SAM, 0.8072\",\n",
    "#    \"Seed 42, SGD, Residual 0.8499\",\n",
    "    \"Seed 42, SGD, 0.8487\",\n",
    "#    \"Seed 11, SAM, Residual 0.8079\",\n",
    "    \"Seed 11, SAM, 0.8054\",\n",
    "#    \"Seed 11, SGD, Residual 0.8519\",\n",
    "    \"Seed 11, SGD, 0.8509\",\n",
    "    \n",
    "#    \"Seed 6, SAM, Residual 0.0.8062\",\n",
    "    \"Seed 6, SAM, 0.8062\",\n",
    "#    \"Seed 6, SGD, Residual 0.8504\",\n",
    "    \"Seed 6, SGD, 0.8503\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c38217958ac5665",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from helper.visualization import Run\n",
    "\n",
    "runs = []\n",
    "for run_id in run_ids:\n",
    "    runs.append(Run(run_id, dataset_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61603b718488c842",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Trainings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a03b1ff594eef0",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in runs:\n",
    "    run.plot_training_records()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a1eb1c7a6c77d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PHATE Embedding Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788f954-3dc5-4e3e-bb0c-68e481ddcad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper.visualization import mphate_on_runs\n",
    "\n",
    "animations = mphate_on_runs(runs, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a7917-9bc7-4a11-a1fe-edfa446b119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.plots import soft_smooth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd64ba-f2a7-4c3c-a8f7-9a6b12cea1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "%matplotlib widget\n",
    "\n",
    "plot_phate_animations(animations, smooth_window=7, smooth_alpha=0.85, start_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb83c06-7e6b-4cda-9e69-bbde9dbfc039",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised = animations[0].denoise(window_size=5, blend=0.7, do_embedding_drift=True, do_cka_similarities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1565260-47ea-4bd5-8067-187666b4384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(animations[0].projections))\n",
    "animations[0].projections[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64286719-b57c-4300-a8b8-a02cc57e630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(denoised.projections))\n",
    "denoised.projections[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a7c1e-3fbe-4220-aeea-4aa3671f9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023343af-b8d1-416a-951c-10154aff2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "print(f\"{titles[n]}\\n\")\n",
    "animations[n].evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5479e81-9694-449f-8720-d65687a4a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_drifts, list_cka = [], [] \n",
    "\n",
    "for i, run in enumerate(titles):\n",
    "    print(f\"{titles[i]}\\n\")\n",
    "    drift, cka = animations[i].evaluate(verbose=False)\n",
    "    list_drifts.append(drift); list_cka.append(cka)\n",
    "    print(f\"Mean Drift Similarity: {drift}, Mean Similarity to CKA: {cka}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc24309-7f0c-4707-ad3d-d4ef25e7e961",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prediction Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc343674-5cb8-42f0-947c-4fb92c8fabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.visualization import compute_prediction_similarities\n",
    "from helper.plots import plot_prediction_similarity_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e96e3-2d7c-44c0-8f93-a633cc700f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = compute_prediction_similarities(runs, similarity=\"cosine\")\n",
    "plot_prediction_similarity_heatmap(similarities, run_titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86aecf-0570-49f6-8f95-724554e0cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408fb174-0fb1-4981-bef0-6afe2c7a979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = compute_prediction_similarities(runs[::-2], similarity=\"cosine\")\n",
    "plot_prediction_similarity_heatmap(similarities, run_titles=titles[::-2], figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76713fc5-cac0-42c7-ae38-776dd5e1c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.visualization import mphate_on_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91b411-0255-40ec-ba0e-b12109a7f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_animations = mphate_on_predictions(runs, titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764ba7d-dbd8-4963-b2c6-adf30dd921e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_phate_animations(pred_animations, start_epoch=5, legend_dist=-1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49823885-c33e-41aa-8551-dd67419a7c55",
   "metadata": {},
   "source": [
    "# PHATE on Model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8c3ea-5295-4186-bfa2-f4884f903ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.visualization import mphate_on_model_weights\n",
    "\n",
    "animations_model = mphate_on_model_weights(runs, titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a70f1b-54d2-4fb1-be80-6af4435af505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: smooth for visualization only\n",
    "plot_phate_animations(animations_model, smooth_window=5, smooth_alpha=0.9, start_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f44a47-1a64-4a8e-8d77-d814d73c48fc",
   "metadata": {},
   "source": [
    "# Linear Mode Connectivity (LMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60773540-3951-4cd3-b210-2362ce71bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a4d6c078-ffa1-4c40-85ec-d3b210c2c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from helper.neuro_viz import compute_lmc_loss_path\n",
    "from helper.visualization import linear_mode_connectivity_path\n",
    "\n",
    "def compute_LMC(runs, idx_1, idx_2, titles, model, dataset_name, device='cpu', num_points=10, ):\n",
    "    print(f\"Compute LMC between ({titles[idx_1]}) and ({titles[idx_2]})\")\n",
    "\n",
    "    info1 = runs[idx_1].results[\"model_info\"]\n",
    "    info2 = runs[idx_2].results[\"model_info\"]\n",
    "    \n",
    "    assert info1 == info2, f\"Model architectures differ:\\n{info1}\\n{info2}\"\n",
    "    print(\"Same architecture:\")\n",
    "    print(runs[idx_1].results[\"model_info\"])\n",
    "\n",
    "    assert repr(model_arch) == runs[idx_1].results[\"model_info\"], \"Wrong model\"\n",
    "\n",
    "    path = linear_mode_connectivity_path(runs[idx_1], runs[idx_2], num_points=num_points)\n",
    "\n",
    "    losses = compute_lmc_loss_path(path, model, dataset_name, device=device)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "afd7af5e-d457-4629-9d90-c19e2aa6c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.vision_classification import init_mlp_for_dataset, init_cnn_for_dataset\n",
    "\n",
    "model_arch = init_cnn_for_dataset(dataset_name, conv_dims=[64, 128, 256], kernel_sizes=[5, 3, 3], hidden_dims=[256, 128], dropout=0.2, residual=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a920cd8c-472d-4d35-a798-ff9d10fe8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmc_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bf44e9cb-8481-4ad1-a98c-c8cbb53ab94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing LMC for runs 0 → 1\n",
      "Compute LMC between (Seed 42, SAM, 0.8072) and (Seed 42, SGD, 0.8487)\n",
      "Same architecture:\n",
      "CNN(conv_dims=[64, 128, 256], kernel_sizes=[5, 3, 3], hidden_dims=[256, 128], dropout=0.2, residual=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute LMC losses:  33%|███▎      | 1/3 [00:42<01:24, 42.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute LMC losses:  33%|███▎      | 1/3 [00:53<01:46, 53.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing LMC for runs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m lmc_results[(i, j)] \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_LMC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_arch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[91], line 18\u001b[0m, in \u001b[0;36mcompute_LMC\u001b[1;34m(runs, idx_1, idx_2, titles, model, dataset_name, device, num_points)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mrepr\u001b[39m(model_arch) \u001b[38;5;241m==\u001b[39m runs[idx_1]\u001b[38;5;241m.\u001b[39mresults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_info\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m path \u001b[38;5;241m=\u001b[39m linear_mode_connectivity_path(runs[idx_1], runs[idx_2], num_points\u001b[38;5;241m=\u001b[39mnum_points)\n\u001b[1;32m---> 18\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_lmc_loss_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses\n",
      "Cell \u001b[1;32mIn[90], line 23\u001b[0m, in \u001b[0;36mcompute_lmc_loss_path\u001b[1;34m(weight_path, model, dataset_name, device, loss_name, whichloss, bn_recal_batches)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     22\u001b[0m     model \u001b[38;5;241m=\u001b[39m repopulate_model_fixed(weights\u001b[38;5;241m.\u001b[39mclone(), model)\n\u001b[1;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhichloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m     25\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mG:\\train-viz\\helper\\neuro_viz.py:280\u001b[0m, in \u001b[0;36mLoss.get_loss\u001b[1;34m(self, dnn, loss_name, whichloss)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m--> 280\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mdnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;66;03m#loss += F.nll_loss(output, target.to(self.device), reduction='sum').item()\u001b[39;00m\n\u001b[0;32m    282\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(output, target\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\.conda\\envs\\phate-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\phate-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mG:\\train-viz\\models\\cnn.py:57\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x, return_embedding)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, return_embedding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     59\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layers(x)\n",
      "File \u001b[1;32m~\\.conda\\envs\\phate-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\phate-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\phate-env\\lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\phate-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\phate-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mG:\\train-viz\\models\\cnn.py:86\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     84\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     85\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m---> 86\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out))\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Apply residual connection\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\phate-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\phate-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\phate-env\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\phate-env\\lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1426\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        if i == j:\n",
    "            continue\n",
    "        # skip if already computed in either order\n",
    "        if (i, j) in lmc_results or (j, i) in lmc_results:\n",
    "            continue\n",
    "        print(f\"Computing LMC for runs {i} → {j}\")\n",
    "        lmc_results[(i, j)] = compute_LMC(\n",
    "            runs, i, j, titles, model_arch, dataset_name, device, num_points=3\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b26ee0-6a0f-4294-b621-b400e20c5b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.array(lmc_results[(0,1)]).min(), np.array(lmc_results[(0,1)]).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40048d1-36f8-4628-8212-66cc3acca14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# lmc_results is a dict mapping (i,j) → list of losses\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for (i, j), losses in lmc_results.items():\n",
    "    ax.plot(\n",
    "        losses,\n",
    "        label=f\"{titles[i]} ↔ {titles[j]}\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Interpolation index\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"LMC loss paths for all run‐pairs\")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.legend(\n",
    "    title=\"Run pairs\",\n",
    "    loc=\"upper right\",\n",
    "    bbox_to_anchor=(1.3, 1),\n",
    "    ncol=1,\n",
    "    frameon=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8a0f8d8e-0546-449c-a1a7-4e391ebe6882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 42, SAM, 0.8072 :  0.58981925\n",
      "Seed 42, SGD, 0.8487 :  0.46490496\n",
      "Seed 11, SAM, 0.8054 :  0.59002\n",
      "Seed 11, SGD, 0.8509 :  0.46150622\n",
      "Seed 6, SAM, 0.8062 :  0.5910221\n",
      "Seed 6, SGD, 0.8503 :  0.45945826\n"
     ]
    }
   ],
   "source": [
    "for i, run in enumerate(runs):\n",
    "    print(titles[i], \": \", run.results[\"val_losses\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cc1b723a-2f54-45e9-a7ff-076ebc642ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from helper.neuro_viz import Loss, repopulate_model_fixed\n",
    "\n",
    "def compute_lmc_loss_path(\n",
    "    weight_path: list[np.ndarray],\n",
    "    model: torch.nn.Module,\n",
    "    dataset_name: str,\n",
    "    device: str = 'cpu',\n",
    "    loss_name: str = 'test_loss',\n",
    "    whichloss: str = 'crossentropy',\n",
    "    bn_recal_batches: int = 100\n",
    "):\n",
    "    loss_obj = Loss(dataset_name, device)\n",
    "    losses = []\n",
    "    model.to(device)\n",
    "\n",
    "    for flat in tqdm(weight_path, desc=\"Compute LMC losses\"):\n",
    "        weights = torch.as_tensor(flat, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            model = repopulate_model_fixed(weights.clone(), model)\n",
    "        loss = loss_obj.get_loss(model, loss_name, whichloss).detach()\n",
    "        print(loss)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd9886-5f86-487d-8d8c-5802f73e69f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phate-env",
   "language": "python",
   "name": "phate-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
